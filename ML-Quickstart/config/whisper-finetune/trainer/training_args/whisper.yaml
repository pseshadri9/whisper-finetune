_target_: transformers.Seq2SeqTrainingArguments
output_dir: ${hydra:runtime.output_dir}

per_device_train_batch_size: 72
per_device_eval_batch_size: 72
dataloader_num_workers: 4
gradient_accumulation_steps: 1
learning_rate: 1e-5
warmup_steps: 500
max_steps: 5000
gradient_checkpointing: True
fp16: True
evaluation_strategy: steps
predict_with_generate: True
generation_max_length: 225
save_steps: 500
eval_steps: 500
logging_steps: 25
report_to: tensorboard
load_best_model_at_end: True
metric_for_best_model: wer
greater_is_better: False
push_to_hub: False